description: Finetune GPT2 on GLUE

target:
  service: amlk8s
  name: itplabrr1cl1
  vc: resrchvc

# option to add key for apt: curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
# install fairseq requirements to docker image
# image is older version of pytorch because newer python version has threading error
environment:
  image: nikghosh09/transformers:latest
  registry: docker.io # any public registry can be specified here
  setup:
    - export HF_DATASETS_CACHE="/mnt/default/huggingface/datasets"
    - pip install tqdm==4.62.3 --user

code:
  # local directory of the code. this will be uploaded to the server.
  # $CONFIG_DIR is expanded to the directory of this config file
  local_dir: $CONFIG_DIR/transformers_v2

data:
  local_dir: ~/.cache/huggingface
  remote_dir: huggingface

# list of jobs to run
jobs:
- name: finetune_mnli
  sku: G1
  preemptible: True
  command:
  - python run_glue.py
    --model_name_or_path /mnt/default/huggingface/gpt2
    --task_name mnli
    --do_train
    --do_eval
    --fp16
    --max_seq_length 128
    --per_device_train_batch_size 32
    --per_device_eval_batch_size 512
    --eval_steps 5000
    --learning_rate 3e-5
    --max_steps 34000
    --output_dir $$AMLT_OUTPUT_DIR
    --evaluation_strategy steps
    --save_strategy no
    --seed 1
    --report_to tensorboard
    --overwrite_output_dir
    --ignore_mismatched_sizes
- name: finetune_qqp
  sku: G1
  preemptible: True
  command:
  - python run_glue.py
    --model_name_or_path /mnt/default/huggingface/gpt2
    --task_name qqp
    --do_train
    --do_eval
    --fp16
    --max_seq_length 128
    --per_device_train_batch_size 32
    --per_device_eval_batch_size 512
    --eval_steps 50
    --learning_rate 2e-5
    --max_steps 200
    --output_dir $$AMLT_OUTPUT_DIR
    --evaluation_strategy steps
    --save_strategy no
    --seed 1
    --report_to tensorboard
    --overwrite_output_dir
    --ignore_mismatched_sizes
